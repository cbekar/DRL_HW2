{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "In this homework, you will be implementing Deep Q Networks and Asynchronous Advantage Actor-Critic models and use them in two of the Atari games and simple gym environments. You will need to fill the missing parts in the modules and then run your tests in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Deep Q Networks (50)\n",
    "\n",
    "As seen in the class, DQN has two main features, namely target networks and replay buffer. However, additional improvements have been introduced since the first release of DQN.\n",
    "\n",
    "- [Prioritized Replay Buffer](https://arxiv.org/pdf/1509.06461.pdf)\n",
    "- [Double Deep Q Networks](https://arxiv.org/pdf/1511.05952.pdf)\n",
    "- [Dueling Deep Q Networks](https://arxiv.org/pdf/1511.06581.pdf)\n",
    "\n",
    "After implementing these methods, you can compare combined algorithm with the vanilla DQN. For comparison you will be using two [gym](https://gym.openai.com/) environments. \n",
    "- [Lunar Lander](LunarLander-v2)\n",
    "- [Pong](https://gym.openai.com/envs/Pong-v0/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to use modified modules without restarting\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch; \n",
    "from gym.version import VERSION\n",
    "print(torch.__version__)\n",
    "print(VERSION)\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from blg604ehw2.dqn import episodic_test\n",
    "from blg604ehw2.dqn import episodic_train\n",
    "\n",
    "from blg604ehw2.network import Network\n",
    "from blg604ehw2.network import FcNet\n",
    "from blg604ehw2.network import DuelingHead\n",
    "from blg604ehw2.network import SimpleHead\n",
    "\n",
    "from blg604ehw2.utils import comparison\n",
    "from blg604ehw2.utils import LoadingBar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lunar Lander with DQN (15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traindqn(env, agent, args, test_rate=1):\n",
    "    bar = LoadingBar(args.episode, \"Episode\")\n",
    "    #agent.to(args.device)\n",
    "    agent.device = args.device\n",
    "    epsilons = np.linspace(args.max_epsilon , args.min_epsilon, num=args.episode)\n",
    "    time_step = 0\n",
    "    rewards = []\n",
    "    td_errors = []\n",
    "\n",
    "    best_model = None\n",
    "    best_rewards = []\n",
    "    best_reward = -np.inf\n",
    "    #import pdb;pdb.set_trace()\n",
    "    for eps in range(args.episode):\n",
    "        ### YOUR CODE HERE ###\n",
    "        time_step, td_error = episodic_train(env, agent, args, epsilons[eps])\n",
    "        if eps % test_rate == 0:\n",
    "            reward = 0\n",
    "            for it in range(test_rate):\n",
    "                reward += episodic_test(env, agent, args)\n",
    "            reward /= test_rate\n",
    "            best_reward = reward if best_reward < reward else best_reward\n",
    "        best_rewards.append(best_reward)\n",
    "        td_errors.append(td_error)\n",
    "        rewards.append(reward)\n",
    "        best_model = agent if best_reward == reward else best_model\n",
    "        \n",
    "        ###       END      ###\n",
    "        bar.progress(eps, best_reward)\n",
    "        \n",
    "    bar.success(best_reward)\n",
    "    return rewards, best_rewards, td_errors, time_step, best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Vanilla DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blg604ehw2.dqn import DQN\n",
    "from blg604ehw2.dqn import ArgsDQN\n",
    "\n",
    "# Set the environment name, you can try differnet environments.\n",
    "envname = \"LunarLander-v2\"\n",
    "\n",
    "# Hyperparameters for the traning\n",
    "# You need to finetune some of the  hyperparameters!\n",
    "env = gym.make(envname)\n",
    "dqn_args = ArgsDQN(\n",
    "        **dict(\n",
    "            env_name=envname,           # Name of the environment\n",
    "            nstates=env.observation_space.shape,  \n",
    "            nact=env.action_space.n,    # Number of actions\n",
    "            buffersize=20000,           # Size of the replay buffer\n",
    "            max_epsilon=0.9,            # Starting value of the epsilon\n",
    "            min_epsilon=0.1,            # Convergence value of the epsilon\n",
    "            target_update_period=50,    # Update period of the target network\n",
    "            gamma=0.99,                 # Discount rate\n",
    "            lr=0.001,                   # Learning rate\n",
    "            device=\"cuda\",              # Device name\n",
    "            batch_size=128,             # Batch size\n",
    "            episode=10,             # Number of episodes for training\n",
    "            max_eps_len=600          # Maximum number of time steps in an episode\n",
    "        )\n",
    "    )\n",
    "\n",
    "def dqn_agent():\n",
    "    # Network construction\n",
    "    feature_net = FcNet(env.observation_space.shape[0], 128)\n",
    "    head_net = SimpleHead(env.action_space.n, 128)\n",
    "    valuenet = Network(feature_net, head_net)\n",
    "    \n",
    "    # Initialize and return agent\n",
    "    return DQN(\n",
    "        dev = dqn_args.device,\n",
    "        valuenet = valuenet,\n",
    "        nact = env.action_space.n,\n",
    "        lr = dqn_args.lr,\n",
    "        buffer_capacity = dqn_args.buffersize,\n",
    "        target_update_period = dqn_args.target_update_period\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "REPEAT = 1\n",
    "lunar_dqn = []\n",
    "#with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "for r in range(REPEAT):\n",
    "    env = gym.make(dqn_args.env_name)\n",
    "    agent = dqn_agent()\n",
    "    lunar_dqn.append(traindqn(env, agent, dqn_args, test_rate=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof.table(sort_by=\"cuda_time_total\")\n",
    "prof.key_averages()\n",
    "prof.export_chrome_trace(\"/home/cbekar/Desktop/trace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plot_texts = [\n",
    "    [\n",
    "        \"Episodic Reward\",\n",
    "        \"episode\",\n",
    "        \"reward\"\n",
    "    ],\n",
    "    [\n",
    "        \"Episodic Best Reward\",\n",
    "        \"episode\",\n",
    "        \"reward\"\n",
    "    ],\n",
    "    [\n",
    "        \"Td Error\",\n",
    "        \"episode\",\n",
    "        \"td\"\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison((lunar_dqn, \"DQN\"), texts = plot_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember that due to the stochasticty of the\n",
    "# environment it may perform differently for each run\n",
    "\n",
    "# Assuming ddpdqn works better in your environment as it should be\n",
    "#import pdb;pdb.set_trace()\n",
    "best_agent_index = max(range(len(lunar_dqn)), key = lambda i: lunar_dqn[i][1][-1])\n",
    "best_agent_state_dict = lunar_dqn[best_agent_index][4]\n",
    "best_agent = dqn_agent()\n",
    "########!best_agent.load_state_dict(best_agent_state_dict)\n",
    "\n",
    "# Monitor saves the mp4 files under \"monitor\" folder.\n",
    "monitor_path = \"LunarLander/DQN/\" + str(dqn_args.episode) + \" episode\"\n",
    "model_path = \"monitor/LunarLander/model_state_dict\"\n",
    "episodic_test(agent=lunar_dqn[best_agent_index][4], env=gym.make(dqn_args.env_name), args=dqn_args, monitor_path=monitor_path)\n",
    "torch.save(best_agent_state_dict, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_path = \"LunarLander/DQN/\" + str(dqn_args.episode) + \" episode\"\n",
    "model_path = \"model_state_dict\"\n",
    "best_agent = torch.load(model_path)\n",
    "episodic_test(agent=best_agent, env=gym.make(dqn_args.env_name), args=dqn_args, monitor_path=monitor_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Dueling Double Prioritized DQN (15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blg604ehw2.dqn import DuelingDoublePrioritizedDQN\n",
    "from blg604ehw2.dqn import ArgsDDPQN\n",
    "\n",
    "\n",
    "# Hyperparameters for the traning\n",
    "# You need to finetune some of the  hyperparameters!\n",
    "envname = \"LunarLander-v2\"\n",
    "env = gym.make(envname)\n",
    "ddpdqn_args = ArgsDDPQN(\n",
    "        **dict(\n",
    "            env_name=envname,           # Name of the environment\n",
    "            nstates=env.observation_space.shape,\n",
    "            nact=env.action_space.n,    # Number of actions\n",
    "            buffersize=20000,           # Size of the replay buffer\n",
    "            max_epsilon=0.9,            # Starting value of the epsilon\n",
    "            min_epsilon=0.1,            # Convergence value of the epsilon\n",
    "            target_replace_period=50,   # Update period of the target network\n",
    "            gamma=0.99,                 # Discount rate\n",
    "            lr=0.001,                   # Learning rate\n",
    "            device=\"cuda\",               # Device name\n",
    "            batch_size=128,             # Batch size\n",
    "            episode=10,                 # Number of episodes for training\n",
    "            max_eps_len=400             # Maximum number of time steps in an episode\n",
    "        )\n",
    "    )\n",
    "\n",
    "def ddpdqn_agent():\n",
    "    # Network construction\n",
    "    feature_net = FcNet(env.observation_space.shape[0], 128)\n",
    "    head_net = DuelingHead(env.action_space.n, 128)\n",
    "    valuenet = Network(feature_net, head_net)\n",
    "\n",
    "    # Initialize agent\n",
    "    return DuelingDoublePrioritizedDQN(\n",
    "        dev = ddpdqn_args.device,\n",
    "        valuenet = valuenet,\n",
    "        nact = env.action_space.n,\n",
    "        lr = ddpdqn_args.lr,\n",
    "        buffer_capacity = ddpdqn_args.buffersize,\n",
    "        target_replace_period = ddpdqn_args.target_replace_period,\n",
    "        gamma = ddpdqn_args.gamma\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPEAT = 1\n",
    "lunar_ddpdqn = []\n",
    "for r in range(REPEAT):\n",
    "    env = gym.make(ddpdqn_args.env_name)\n",
    "    agent = ddpdqn_agent()\n",
    "    lunar_ddpdqn.append(traindqn(env, agent, ddpdqn_args, test_rate=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison((lunar_ddpdqn, \"DDPDQN\"), (lunar_dqn, \"DQN\"), texts = plot_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may compare these improvements by themselves if you want to. See which one of them makes the most improvement for the Lunar Lander environment.(optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's visualize the best agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember that due to the stochasticty of the\n",
    "# environment it may perform differently for each run\n",
    "\n",
    "# Assuming ddpdqn works better in your environment as it should be\n",
    "best_agent_index = max(range(len(lunar_ddpdqn)), key = lambda i: lunar_ddpdqn[i][1][-1])\n",
    "best_agent_state_dict = lunar_ddpdqn[best_agent_index][4]\n",
    "best_agent = ddpdqn_agent()\n",
    "best_agent.load_state_dict(best_agent_state_dict)\n",
    "\n",
    "# Monitor saves the mp4 files under \"monitor\" folder.\n",
    "monitor_path = \"LunarLander/\" + str(ddpdqn_args.episode) + \" episode\"\n",
    "model_path = \"monitor/LunarLander/model_state_dict\"\n",
    "episodic_test(agent=best_agent, env=gym.make(ddpdqn_args.env_name), args=ddpdqn_args, monitor_path=monitor_path)\n",
    "torch.save(best_agent_state_dict, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pong with Dueling Double Prioritized DQN (20)\n",
    "- This may take long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blg604ehw2.atari_wrapper import ClipRewardEnv\n",
    "from blg604ehw2.atari_wrapper import FrameStack\n",
    "from blg604ehw2.atari_wrapper import EpisodicLifeEnv\n",
    "from blg604ehw2.atari_wrapper import WarpFrame\n",
    "from blg604ehw2.atari_wrapper import ScaledFloatFrame\n",
    "\n",
    "from blg604ehw2.network import Cnn\n",
    "\n",
    "envname = \"PongNoFrameskip-v4\" # Should be without frameskips\n",
    "\n",
    "# Wrapped atari environment.\n",
    "# It is important to use these wrappers in order\n",
    "# to simplfy learning. In their Nature paper,\n",
    "# Deepmind used some of them to achive those\n",
    "# results. It is good to check them and see \n",
    "# what do they do.\n",
    "def pongenv():\n",
    "    env = gym.make(envname)\n",
    "    env = ClipRewardEnv(env)            # Clip the reward between -1 and 1\n",
    "    env = WarpFrame(env)                # Downsample rgb (210, 160, 3) images to gray images (84, 84)\n",
    "    env = EpisodicLifeEnv(env)          # Terminate the environment after a live is lost\n",
    "    env = FrameStack(env, k=4)          # Stack consecutive frames as a single state\n",
    "    return env\n",
    "\n",
    "# Hyperparameters for the traning\n",
    "# This time parameter tunning is even more important!\n",
    "# If you have access to a gpu use it! Set the device accordingly.\n",
    "env = pongenv()\n",
    "pong_args = ArgsDDPQN(\n",
    "        **dict(\n",
    "            env_name=envname,           # Name of the environment\n",
    "            nstates=env.observation_space,\n",
    "            nact=env.action_space.n,    # Number of actions\n",
    "            buffersize=1000,          # Size of the replay buffer\n",
    "            max_epsilon=0.9,            # Starting value of the epsilon\n",
    "            min_epsilon=0.1,            # Convergence value of the epsilon\n",
    "            target_replace_period=100,  # Update period of the target network\n",
    "            gamma=0.97,                 # Discount rate\n",
    "            lr=0.0002,                  # Learning rate\n",
    "            device=\"cuda\",              # Device name\n",
    "            batch_size=128,             # Batch size\n",
    "            episode=10,               # Number of episodes for training\n",
    "            max_eps_len=100            # Maximum number of time steps in an episode\n",
    "        )\n",
    "    )\n",
    "\n",
    "def pong_agent():\n",
    "    # Network construction\n",
    "    feature_net = Cnn(4, 512)\n",
    "    head_net = DuelingHead(env.action_space.n, 512)\n",
    "    valuenet = Network(feature_net, head_net)\n",
    "\n",
    "    # Initialize agent\n",
    "    return DuelingDoublePrioritizedDQN(\n",
    "        dev = pong_args.device,\n",
    "        valuenet = valuenet,\n",
    "        nact = env.action_space.n,\n",
    "        lr = pong_args.lr,\n",
    "        buffer_capacity = pong_args.buffersize,\n",
    "        target_replace_period = pong_args.target_replace_period\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "REPEAT = 1 # Assign 1 if you dont want to train more than one\n",
    "pong_ddpdqn = []\n",
    "for r in range(REPEAT):\n",
    "    env = pongenv()\n",
    "    agent = pong_agent()\n",
    "    pong_ddpdqn.append(traindqn(env, agent, pong_args, test_rate=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison((pong_ddpdqn, \"PONG\"), texts = plot_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_agent_index = max(range(len(pong_ddpdqn)), key = lambda i: pong_ddpdqn[i][1][-1])\n",
    "best_agent_state_dict = pong_ddpdqn[best_agent_index][4]\n",
    "best_agent = pong_agent() \n",
    "#best_agent.load_state_dict(best_agent_state_dict)\n",
    "\n",
    "# Monitor saves the mp4 files under \"monitor\" folder.\n",
    "monitor_path = \"Pong/\" + str(pong_args.episode) + \" episode\"\n",
    "model_path = \"monitor/Pong/model_state_dict\"\n",
    "episodic_test(agent=pong_ddpdqn[best_agent_index][4], env=pongenv(), args=pong_args, monitor_path=monitor_path)\n",
    "torch.save(best_agent_state_dict, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Asynchronous Advantage Actor-Critic (50)\n",
    "[A3C](https://arxiv.org/abs/1602.01783) is a policy gradinet algorithm which is based on asynchronous updates of paralel agents.\n",
    "You will be testing your agent in:\n",
    "\n",
    "- [Bipedal Walker](https://gym.openai.com/envs/BipedalWalker-v2/)\n",
    "- [Breakout](https://gym.openai.com/envs/Breakout-v0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Bipedal Walker with Asynchronous Advantage Actor-Critic (20)\n",
    "It is important to test your implementation with a simpler enviroment like BipedalWalker before trying Breakout.\n",
    "It is highly recommended to check the pseudocode in the paper's appendix.\n",
    "\n",
    "The implementation works as follows:\n",
    "\n",
    "    - Create a global agent which's paremeters are in the shared memory.\n",
    "    - Create multiple worker processes. That performs:\n",
    "        - Gradient calculation with the transition it observed\n",
    "        - Update the global agent with the gradients\n",
    "        - Synchronize with the global agent\n",
    "    - Create a test process that evaluates the performance of the global agent over the course of the training\n",
    "    - Run these workers asynchronously\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to use modified modules without restarting\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import namedtuple\n",
    "import torch.multiprocessing as mp \n",
    "\n",
    "from blg604ehw2.network import FcNet\n",
    "from blg604ehw2.network import ContinuousDistHead\n",
    "from blg604ehw2.network import Network\n",
    "\n",
    "from blg604ehw2.a3c import ContinuousA3c\n",
    "from blg604ehw2.a3c import SharedAdam\n",
    "from blg604ehw2.a3c import train_worker\n",
    "from blg604ehw2.a3c import test_worker\n",
    "from blg604ehw2.a3c import A3C_args\n",
    "\n",
    "\n",
    "# Bipedal Walker environment is similar to Lunar Lander\n",
    "# State space is a vector of length 24 and there are\n",
    "# 4 actions\n",
    "envname = \"BipedalWalker-v2\"\n",
    "\n",
    "# Logger is a named tuple of shared lists integer and a model\n",
    "# It is necessary to have a shared object since it can be used\n",
    "# by many processes\n",
    "Logger = namedtuple(\"Logger\", \"eps_reward best_reward best_model time_steps time\")\n",
    "\n",
    "# Hyperparameters, again tunning is necessary but optional.\n",
    "a3c_args = A3C_args(\n",
    "    **dict(\n",
    "        maxtimestep=100000,     # Number of time steps for training\n",
    "        maxlen=600,             # Maximum length of an episode\n",
    "        nstep=20,               # Bootsrapping length (n-step td)\n",
    "        gamma=0.98,             # Discount rate\n",
    "        lr=0.0001,              # Learning rate\n",
    "        beta=0.01,              # Entropy regularization constant\n",
    "        device=\"cpu\",           # Device\n",
    "    )\n",
    ")\n",
    "\n",
    "# Agent generating function\n",
    "def a3c_agent():\n",
    "    feature_net = FcNet(24)\n",
    "    head_net = ContinuousDistHead(128, 4)\n",
    "    network = Network(feature_net, head_net)\n",
    "    agent = ContinuousA3c(network)\n",
    "    agent.device = a3c_args.device\n",
    "    return agent\n",
    "\n",
    "# Environment generating function\n",
    "# You can use RewardClip wrapper\n",
    "def walker_env():\n",
    "    env =  gym.make(envname)\n",
    "    return env\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Main cell for Bipedal Walker ###\n",
    "\n",
    "# Number of training workers\n",
    "N_PROCESSES = mp.cpu_count()\n",
    "\n",
    "# Global agent that will be used for synchronization.\n",
    "global_agent = a3c_agent()\n",
    "global_agent.share_memory()         # Make sure it is in the shared memory!\n",
    "\n",
    "# Shared optimizer, since the optimizer has its own parameters\n",
    "# they need to be in the shared memory as well.\n",
    "sharedopt = SharedAdam(global_agent.parameters(), lr=a3c_args.lr)\n",
    "\n",
    "# Another agent for logging purposes\n",
    "best_agent = a3c_agent()\n",
    "best_agent.share_memory()\n",
    "\n",
    "# Logger\n",
    "# Manager controls another process(server process) to share\n",
    "# objects between multiple processes via proxies.\n",
    "# Please read https://docs.python.org/3.7/library/multiprocessing.html\n",
    "# for more information.\n",
    "manager = mp.Manager()\n",
    "logger = Logger(\n",
    "    manager.list(),\n",
    "    manager.list(),\n",
    "    best_agent,\n",
    "    manager.list(),\n",
    "    manager.Value(\"i\", 0)\n",
    ")\n",
    "logger.time_steps.append(0)\n",
    "for t in range(a3c_args.maxtimestep):\n",
    "    logger.eps_reward.append(None)\n",
    "    logger.best_reward.append(None)\n",
    "    \n",
    "# Lock is not necessary\n",
    "lock = mp.Lock()\n",
    "\n",
    "# Start by creating a test worker\n",
    "processes = []\n",
    "process = mp.Process(target=test_worker,\n",
    "                     args=(a3c_args, global_agent, walker_env, a3c_agent, lock, logger,\\\n",
    "                           None,False))\n",
    "# test_worker(a3c_args, global_agent, walker_env, a3c_agent, N_PROCESSES, logger)\n",
    "# train_worker(a3c_args, global_agent, sharedopt, walker_env, a3c_agent, 0, N_PROCESSES, logger)\n",
    "process.start()\n",
    "processes.append(process)\n",
    "\n",
    "# # Train workers\n",
    "for t in range(N_PROCESSES):\n",
    "    process = mp.Process(target=train_worker,\n",
    "                         args=(a3c_args, global_agent, sharedopt, walker_env, a3c_agent, t, N_PROCESSES, logger))\n",
    "    process.start()\n",
    "    processes.append(process)\n",
    "    \n",
    "# Wait until all done\n",
    "for p in processes:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the rewards from the logger\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title(\"A3C Walker Best Rewards\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.xlabel(\"time steps\")\n",
    "plt.plot(list(logger.time_steps), list(logger.best_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Breakout with Asynchronous Advantage Actor-Critic (30)\n",
    "\n",
    "- It takes a lot of time so be prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to use modified modules without restarting\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "from blg604ehw2.atari_wrapper import ClipRewardEnv\n",
    "from blg604ehw2.atari_wrapper import FrameStack\n",
    "from blg604ehw2.atari_wrapper import EpisodicLifeEnv\n",
    "from blg604ehw2.atari_wrapper import WarpFrame\n",
    "from blg604ehw2.atari_wrapper import ScaledFloatFrame\n",
    "\n",
    "from blg604ehw2.network import Cnn\n",
    "from blg604ehw2.network import DiscreteDistHead\n",
    "from collections import namedtuple\n",
    "import torch.multiprocessing as mp \n",
    "from blg604ehw2.a3c import DiscreteA3c\n",
    "from blg604ehw2.a3c import A3C_args\n",
    "from blg604ehw2.network import Network\n",
    "from blg604ehw2.a3c import SharedAdam\n",
    "from blg604ehw2.a3c import train_worker\n",
    "from blg604ehw2.a3c import test_worker\n",
    "# Breakout Environment\n",
    "envname = \"Breakout-v4\"\n",
    "Logger = namedtuple(\"Logger\", \"eps_reward best_reward best_model time_steps time\")\n",
    "\n",
    "# Hyperparameters\n",
    "breakout_args = A3C_args(\n",
    "    **dict(\n",
    "        maxtimestep=40000000,\n",
    "        maxlen=2000,\n",
    "        nstep=20,\n",
    "        gamma=0.98,\n",
    "        lr=0.00003,\n",
    "        beta=0.01,\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")\n",
    "\n",
    "def breakout_agent():\n",
    "    feature_net = Cnn(4, 512) # 4 channel size because of the StackFrame buffer\n",
    "    head_net = DiscreteDistHead(512, 4) # 4 output because of the environment's action space\n",
    "    network = Network(feature_net, head_net)\n",
    "    agent = DiscreteA3c(network)\n",
    "    agent.device = breakout_args.device\n",
    "    return agent\n",
    "\n",
    "def breakout_env():\n",
    "    env = gym.make(envname)\n",
    "    env = ClipRewardEnv(env)            # Clip the reward between -1 and 1\n",
    "    env = WarpFrame(env)                # Downsample rgb (210, 160, 3) images to gray images (84, 84)\n",
    "    env = EpisodicLifeEnv(env)          # Terminate the environment after a live is lost\n",
    "    env = FrameStack(env, k=4)          # Stack consecutive frames as a single state\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Main cell for Breakout ###\n",
    "\n",
    "N_PROCESSES = mp.cpu_count()\n",
    "\n",
    "global_agent = breakout_agent()\n",
    "global_agent.share_memory()\n",
    "sharedopt = SharedAdam(global_agent.parameters(), lr=breakout_args.lr)\n",
    "\n",
    "best_agent = breakout_agent()\n",
    "best_agent.share_memory()\n",
    "\n",
    "# Try to use one manager\n",
    "manager = mp.Manager()\n",
    "logger = Logger(\n",
    "    manager.list(),\n",
    "    manager.list(),\n",
    "    best_agent,\n",
    "    manager.list(),\n",
    "    manager.Value(\"i\", 0)\n",
    ")\n",
    "logger.time_steps.append(0)\n",
    "for t in range(N_PROCESSES):\n",
    "    logger.eps_reward.append(None)\n",
    "    logger.best_reward.append(None)\n",
    "lock = mp.Lock()\n",
    "\n",
    "processes = []\n",
    "\n",
    "process = mp.Process(target=test_worker,\n",
    "                     args=(breakout_args, global_agent, breakout_env, breakout_agent, lock, logger))\n",
    "# train_worker(breakout_args, global_agent, sharedopt, breakout_env, breakout_agent, 0, N_PROCESSES, logger)\n",
    "# test_worker(breakout_args, global_agent, breakout_env, breakout_agent, N_PROCESSES, logger)\n",
    "process.start()\n",
    "processes.append(process)\n",
    "for t in range(N_PROCESSES):\n",
    "    process = mp.Process(target=train_worker,\n",
    "                         args=(breakout_args, global_agent, sharedopt, breakout_env, breakout_agent, t, N_PROCESSES, logger))\n",
    "    process.start()\n",
    "    processes.append(process)\n",
    "for p in processes:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the rewards from the logger\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title(\"A3C Breakout Best Rewards\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.xlabel(\"time steps\")\n",
    "plt.plot(list(logger.time_steps), list(logger.best_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model's parameters\n",
    "model_path = \"monitor/Breakout/model_state_dict\"\n",
    "torch.save(logger.best_model.state_dict(), model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
